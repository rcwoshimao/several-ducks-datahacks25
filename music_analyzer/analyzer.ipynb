{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate # For metrics\n",
    "from datasets import DatasetDict, Audio, Dataset\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "CSV_PATH = (DATA_DIR / \"musiccaps-public.csv\").resolve()\n",
    "AUDIO_DIR = DATA_DIR / \"audio\" \n",
    "AUDIO_EXTENSION = \".wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata using pandas...\n",
      "CSV loaded successfully with pandas. Shape: (5521, 9)\n",
      "Converted pandas DataFrame to datasets.Dataset object:\n",
      "Dataset({\n",
      "    features: ['ytid', 'start_s', 'end_s', 'audioset_positive_labels', 'aspect_list', 'caption', 'author_id', 'is_balanced_subset', 'is_audioset_eval'],\n",
      "    num_rows: 5521\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading metadata using pandas...\")\n",
    "try:\n",
    "    # Step 1: Load the CSV using pandas\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"CSV loaded successfully with pandas. Shape: {df.shape}\")\n",
    "\n",
    "    # Step 2: Convert the pandas DataFrame to a datasets.Dataset object\n",
    "    raw_metadata_dataset = Dataset.from_pandas(df)\n",
    "    print(\"Converted pandas DataFrame to datasets.Dataset object:\")\n",
    "    print(raw_metadata_dataset)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR (pandas): File not found during read_csv at {CSV_PATH}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Failed during pandas load or Dataset conversion from {CSV_PATH}:\")\n",
    "    raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ytid</th>\n",
       "      <th>start_s</th>\n",
       "      <th>end_s</th>\n",
       "      <th>audioset_positive_labels</th>\n",
       "      <th>aspect_list</th>\n",
       "      <th>caption</th>\n",
       "      <th>author_id</th>\n",
       "      <th>is_balanced_subset</th>\n",
       "      <th>is_audioset_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0Gj8-vB1q4</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/0140xf,/m/02cjck,/m/04rlf</td>\n",
       "      <td>['low quality', 'sustained strings melody', 's...</td>\n",
       "      <td>The low quality recording features a ballad so...</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0SdAVK79lg</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/0155w,/m/01lyv,/m/0342h,/m/042v_gx,/m/04rlf...</td>\n",
       "      <td>['guitar song', 'piano backing', 'simple percu...</td>\n",
       "      <td>This song features an electric guitar as the m...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0vPFx-wRRI</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/025_jnm,/m/04rlf</td>\n",
       "      <td>['amateur recording', 'finger snipping', 'male...</td>\n",
       "      <td>a male voice is singing a melody with changing...</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0xzrMun0Rs</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/01g90h,/m/04rlf</td>\n",
       "      <td>['backing track', 'jazzy', 'digital drums', 'p...</td>\n",
       "      <td>This song contains digital drums playing a sim...</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1LrH01Ei1w</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/02p0sh1,/m/04rlf</td>\n",
       "      <td>['rubab instrument', 'repetitive melody on dif...</td>\n",
       "      <td>This song features a rubber instrument being p...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ytid  start_s  end_s  \\\n",
       "0  -0Gj8-vB1q4       30     40   \n",
       "1  -0SdAVK79lg       30     40   \n",
       "2  -0vPFx-wRRI       30     40   \n",
       "3  -0xzrMun0Rs       30     40   \n",
       "4  -1LrH01Ei1w       30     40   \n",
       "\n",
       "                            audioset_positive_labels  \\\n",
       "0                       /m/0140xf,/m/02cjck,/m/04rlf   \n",
       "1  /m/0155w,/m/01lyv,/m/0342h,/m/042v_gx,/m/04rlf...   \n",
       "2                                /m/025_jnm,/m/04rlf   \n",
       "3                                 /m/01g90h,/m/04rlf   \n",
       "4                                /m/02p0sh1,/m/04rlf   \n",
       "\n",
       "                                         aspect_list  \\\n",
       "0  ['low quality', 'sustained strings melody', 's...   \n",
       "1  ['guitar song', 'piano backing', 'simple percu...   \n",
       "2  ['amateur recording', 'finger snipping', 'male...   \n",
       "3  ['backing track', 'jazzy', 'digital drums', 'p...   \n",
       "4  ['rubab instrument', 'repetitive melody on dif...   \n",
       "\n",
       "                                             caption  author_id  \\\n",
       "0  The low quality recording features a ballad so...          4   \n",
       "1  This song features an electric guitar as the m...          0   \n",
       "2  a male voice is singing a melody with changing...          6   \n",
       "3  This song contains digital drums playing a sim...          6   \n",
       "4  This song features a rubber instrument being p...          0   \n",
       "\n",
       "   is_balanced_subset  is_audioset_eval  \n",
       "0               False              True  \n",
       "1               False             False  \n",
       "2               False              True  \n",
       "3               False              True  \n",
       "4               False             False  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Loading Processor for openai/whisper-small ---\n",
      "Loading feature extractor...\n",
      "Loading tokenizer...\n",
      "Loading processor...\n",
      "\n",
      "Processor components loaded successfully:\n",
      "  Feature Extractor: <class 'transformers.models.whisper.feature_extraction_whisper.WhisperFeatureExtractor'>\n",
      "  Tokenizer: <class 'transformers.models.whisper.tokenization_whisper.WhisperTokenizer'>\n",
      "  Processor: <class 'transformers.models.whisper.processing_whisper.WhisperProcessor'>\n",
      "\n",
      "These components are needed to prepare data for the 'openai/whisper-small' model.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Load Processor (Feature Extractor + Tokenizer) ---\n",
    "print(f\"\\n--- Step 2: Loading Processor for {MODEL_NAME} ---\")\n",
    "\n",
    "try:\n",
    "    # Load the feature extractor (processes audio input)\n",
    "    # We load it now even though we skipped audio loading, as it's part of the processor bundle\n",
    "    print(\"Loading feature extractor...\")\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "\n",
    "    print(\"Loading processor...\")\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "\n",
    "    print(\"\\nProcessor components loaded successfully:\")\n",
    "    print(f\"  Feature Extractor: {type(feature_extractor)}\")\n",
    "    print(f\"  Tokenizer: {type(tokenizer)}\")\n",
    "    print(f\"  Processor: {type(processor)}\")\n",
    "    print(f\"\\nThese components are needed to prepare data for the '{MODEL_NAME}' model.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ERROR Loading Processor ---\")\n",
    "    print(f\"Failed to load components for model '{MODEL_NAME}'.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(f\"  - The model name ('{MODEL_NAME}') is correct.\")\n",
    "    print(f\"  - You have an active internet connection to download from Hugging Face Hub.\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    \"\"\"\n",
    "    Prepares a batch of data for the Whisper model.\n",
    "\n",
    "    Input batch EXPECTS:\n",
    "      - An 'audio' column: Containing dictionaries from datasets.Audio. Example:\n",
    "          {'path': '...', 'array': numpy.ndarray, 'sampling_rate': 16000}\n",
    "      - A 'caption' column: Containing the text transcriptions/captions.\n",
    "\n",
    "    Output batch CONTAINS:\n",
    "      - 'input_features': Processed audio data (log-Mel spectrogram) for the model.\n",
    "      - 'labels': Tokenized text caption IDs for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Process Audio ---\n",
    "    # This part relies on the 'audio' column being present and correctly formatted\n",
    "    try:\n",
    "        audio_data = batch[\"audio\"]\n",
    "        # Extract log-Mel spectrogram features from the raw audio array\n",
    "        batch[\"input_features\"] = feature_extractor(\n",
    "            audio_data[\"array\"], sampling_rate=audio_data[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "        # Note: We take [0] because feature_extractor processes one sample at a time\n",
    "\n",
    "    except KeyError:\n",
    "        # This error will likely happen if you call .map(prepare_dataset) NOW\n",
    "        # because the 'audio' column isn't ready.\n",
    "        print(\"ERROR in prepare_dataset: 'audio' column not found or not in the expected format.\")\n",
    "        print(\"--> Ensure audio loading steps (mapping paths, casting to Audio) are completed first! <---\")\n",
    "        batch[\"input_features\"] = None\n",
    "\n",
    "        # You might want to raise an error here if you were actually running this:\n",
    "        # raise ValueError(\"Audio data missing or in wrong format in prepare_dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during audio processing in prepare_dataset: {e}\")\n",
    "        batch[\"input_features\"] = None # Assign None on other errors too\n",
    "\n",
    "    # --- 2. Process Text ---\n",
    "    # This part should work fine now, as 'caption' comes from your CSV.\n",
    "    captions = batch[\"caption\"]\n",
    "\n",
    "    # Basic cleanup: Ensure captions are strings and handle None values.\n",
    "    current_caption = captions # Assuming mapping is not batched for simplicity here\n",
    "    if current_caption is None:\n",
    "        processed_caption = \"\"\n",
    "    else:\n",
    "        processed_caption = str(current_caption)\n",
    "\n",
    "    batch[\"labels\"] = tokenizer(processed_caption).input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ERROR Initializing Data Collator (Attempt 2) ---\n",
      "Passing tokenizer and feature_extractor failed with: __init__() got an unexpected keyword argument 'feature_extractor'\n",
      "Trying fallback: Passing ONLY the tokenizer...\n",
      "\n",
      "DataCollatorForSeq2Seq initialized successfully (using ONLY tokenizer).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "try:\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=processor.tokenizer,          # Use the tokenizer part of the processor\n",
    "        feature_extractor=processor, # Use the feature_extractor part\n",
    "        model=None,                             # Still avoid passing the model instance here\n",
    "        padding=True                            # Enable dynamic padding\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataCollatorForSeq2Seq initialized successfully (using tokenizer/feature_extractor).\")\n",
    "    print(\"This object will be passed to the Trainer later to handle batching and padding.\")\n",
    "\n",
    "    print(\"\\nNext steps before training:\")\n",
    "    print(\"  - Step 5: Define Evaluation Metrics (Optional but recommended)\")\n",
    "    print(\"  - Step 6: Load Pretrained Model (`WhisperForConditionalGeneration`)\")\n",
    "    # ... etc ...\n",
    "\n",
    "\n",
    "except TypeError as e:\n",
    "    # If the above still fails (e.g., feature_extractor is ALSO unexpected)\n",
    "    print(f\"\\n--- ERROR Initializing Data Collator (Attempt 2) ---\")\n",
    "    print(f\"Passing tokenizer and feature_extractor failed with: {e}\")\n",
    "    print(\"Trying fallback: Passing ONLY the tokenizer...\")\n",
    "    try:\n",
    "        # Fallback: Some versions/use-cases might only need the tokenizer\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=processor.tokenizer,\n",
    "            model=None,\n",
    "            padding=True\n",
    "        )\n",
    "        print(\"\\nDataCollatorForSeq2Seq initialized successfully (using ONLY tokenizer).\")\n",
    "\n",
    "    except Exception as e2:\n",
    "         print(f\"\\n--- ERROR Initializing Data Collator (Attempt 3) ---\")\n",
    "         print(f\"Failed even with only the tokenizer: {e2}\")\n",
    "         import traceback\n",
    "         traceback.print_exc()\n",
    "         print(\"\\nCould not initialize DataCollatorForSeq2Seq.\")\n",
    "         print(\"Please check your `transformers` library version.\")\n",
    "         print(\"The expected arguments for DataCollatorForSeq2Seq might differ.\")\n",
    "         print(\"Ensure 'processor.tokenizer' and 'processor.feature_extractor' exist.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ERROR Initializing Data Collator ---\")\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Please ensure the 'processor' object from Step 2 loaded correctly and has .tokenizer / .feature_extractor attributes.\")\n",
    "\n",
    "\n",
    "# If successful, the variable 'data_collator' is now ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ROUGE metric from the 'evaluate' library...\n",
      "ROUGE metric loaded.\n",
      "\n",
      "Function 'compute_metrics' defined successfully.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    print(\"Loading ROUGE metric from the 'evaluate' library...\")\n",
    "    metric = evaluate.load(\"rouge\")\n",
    "    print(\"ROUGE metric loaded.\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        \"\"\"\n",
    "        Computes ROUGE scores from model predictions.\n",
    "\n",
    "        Args:\n",
    "            eval_pred (EvalPrediction): A tuple containing predictions\n",
    "                                      (usually generated token IDs) and label_ids.\n",
    "        \"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        # 'predictions' will be the output token IDs from the model generation\n",
    "        # 'labels' are the ground truth token IDs\n",
    "\n",
    "        # Decode predicted token IDs to text\n",
    "        # Handle potential pad_token_id (though usually generation stops before pad)\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "        # Decode label token IDs to text\n",
    "        # Replace -100 (used for padding in labels during training) with the actual pad token ID\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # ROUGE scoring often works best if sentences are separated by newlines.\n",
    "        # --- Simpler approach: just strip whitespace ---\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        # Compute ROUGE scores\n",
    "        # result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"])\n",
    "        # Explicitly list types if needed, otherwise default ROUGE-L etc. are calculated.\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "        # Multiply scores by 100\n",
    "        result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "        # Optional: Add generated length metric\n",
    "        # prediction_lens = [np.count_nonzero(predictions[i] != tokenizer.pad_token_id) for i in range(predictions.shape[0])]\n",
    "        # result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "        # Round results\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "        return result\n",
    "\n",
    "    print(\"\\nFunction 'compute_metrics' defined successfully.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\n--- ERROR: Package Not Found ---\")\n",
    "    print(\"Skipping metric definition. Evaluation during training will only show loss.\")\n",
    "    compute_metrics = None # Set to None so Trainer doesn't require it\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    print(\"Skipping metric definition.\")\n",
    "    compute_metrics = None # Set to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 'openai/whisper-small' from Hugging Face Hub...\n",
      "No GPU detected. Model will stay on CPU.\n",
      "\n",
      "Model 'openai/whisper-small' loaded successfully.\n",
      "Model class: <class 'transformers.models.whisper.modeling_whisper.WhisperForConditionalGeneration'>\n",
      "Model is on device: cpu\n",
      "Number of parameters: 241.73 M\n",
      "\n",
      "The 'model' object is now ready.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "import torch # Usually needed implicitly by transformers\n",
    "\n",
    "# We use the WhisperForConditionalGeneration class, which is suitable for\n",
    "# sequence-to-sequence tasks like transcription and captioning.\n",
    "# It loads the pre-trained weights identified by MODEL_NAME.\n",
    "\n",
    "try:\n",
    "    print(f\"Loading model '{MODEL_NAME}' from Hugging Face Hub...\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # It's generally good practice to ensure 'use_cache' is True for generation tasks.\n",
    "    # This speeds up decoding during evaluation by reusing past computations.\n",
    "    # It's often the default, but we set it explicitly for clarity.\n",
    "    model.config.use_cache = True\n",
    "\n",
    "    # Check if a GPU is available and move the model to it\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU detected. Moving model to GPU...\")\n",
    "        device = torch.device(\"cuda\")\n",
    "        model.to(device)\n",
    "        print(\"Model moved to GPU.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No GPU detected. Model will stay on CPU.\")\n",
    "\n",
    "    print(f\"\\nModel '{MODEL_NAME}' loaded successfully.\")\n",
    "    print(f\"Model class: {type(model)}\")\n",
    "    print(f\"Model is on device: {model.device}\") # Verify if it's on CPU or GPU\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Number of parameters: {total_params / 1e6:.2f} M\")\n",
    "\n",
    "    print(\"\\nThe 'model' object is now ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ERROR Loading Model: Unexpected Error ---\")\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqTrainingArguments initialized successfully.\n",
      "Checkpoints and logs will be saved to: ./whisper-musiccaps-finetuned-local\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# Define output directory for checkpoints and final model\n",
    "OUTPUT_DIR = \"./whisper-musiccaps-finetuned-local\" # Choose a suitable name\n",
    "\n",
    "try:\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR, # Where to save the model checkpoints and logs\n",
    "        num_train_epochs=3,    # Total number of training epochs (adjust as needed)\n",
    "        per_device_train_batch_size=8, # Batch size per GPU for training (lower if OOM)\n",
    "        per_device_eval_batch_size=8,  # Batch size per GPU for evaluation\n",
    "        learning_rate=1e-5,  # $[1 \\times 10^{-5}]$ - Initial learning rate for AdamW optimizer\n",
    "        weight_decay=0.01,   # Weight decay for regularization\n",
    "        warmup_steps=500,    # Number of steps for linear warmup from 0 to learning_rate\n",
    "\n",
    "        # Logging, Saving, and Evaluation Strategies\n",
    "        logging_dir=f\"{OUTPUT_DIR}/logs\", # Directory for TensorBoard logs\n",
    "        logging_strategy=\"steps\",        # Log metrics every `logging_steps`\n",
    "        logging_steps=25,                # Log every 25 steps\n",
    "        eval_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
    "        # eval_steps=500,                # Evaluate every N steps (use instead of epoch if preferred)\n",
    "        save_strategy=\"epoch\",           # Save checkpoint at the end of each epoch\n",
    "        # save_steps=500,                # Save checkpoint every N steps\n",
    "        save_total_limit=2,              # Limit the total number of checkpoints saved (saves disk space)\n",
    "        \n",
    "        # Performance and Hardware\n",
    "        fp16=True,                       # Enable mixed precision training (requires compatible GPU)\n",
    "        # optim=\"adamw_torch\",           # Use PyTorch's AdamW optimizer (often default)\n",
    "        gradient_accumulation_steps=2,   # Increase effective batch size (train_batch*num_gpu*accum)\n",
    "        # gradient_checkpointing=True,   # Can save memory at cost of slower training (use if OOM)\n",
    "        # Seq2Seq Specific Arguments for Evaluation\n",
    "        predict_with_generate=True,      # MUST be True to generate sequences for ROUGE/BLEU evaluation\n",
    "        generation_max_length=225,       # Max number of tokens to generate during evaluation\n",
    "        \n",
    "        # Model Loading/Saving Control\n",
    "        load_best_model_at_end=True,     # Load the best model checkpoint found during training\n",
    "        metric_for_best_model=\"eval_loss\", # Metric to determine the best model (use 'eval_rougeL' if using ROUGE)\n",
    "        greater_is_better=False,         # False for loss, True for metrics like ROUGE/BLEU\n",
    "        \n",
    "        # Other Settings\n",
    "        remove_unused_columns=False,     # Recommended False when using custom data processing\n",
    "        label_names=[\"labels\"],          # Explicitly name the label column(s)\n",
    "        report_to=[\"tensorboard\"]        # Log to TensorBoard (can add \"wandb\" or \"mlflow\")\n",
    "    )\n",
    "\n",
    "    print(\"Seq2SeqTrainingArguments initialized successfully.\")\n",
    "    print(f\"Checkpoints and logs will be saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ERROR Initializing Training Arguments ---\")\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ERROR Initializing Trainer ---\n",
      "An unexpected error occurred: You have set `args.eval_strategy` to epoch but you didn't pass an `eval_dataset` to `Trainer`. Either set `args.eval_strategy` to `no` or pass an `eval_dataset`. \n",
      "Ensure model, args, collator, tokenizer/processor, compute_metrics are correctly defined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3082353/4017178928.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# !!! IMPORTANT: DATASET PLACEHOLDERS !!!\n",
    "# These MUST be replaced before calling trainer.train().\n",
    "\n",
    "try:\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,                     # The Whisper model ready for fine-tuning\n",
    "        args=training_args,              # Configuration for the training process\n",
    "        data_collator=data_collator,     # Handles padding batches dynamically\n",
    "\n",
    "        # --- Pass the Datasets (Using Placeholders for Now) ---\n",
    "        train_dataset=None,              # <<< MUST be replaced with processed train data\n",
    "        eval_dataset=None,               # <<< MUST be replaced with processed eval data\n",
    "\n",
    "        # --- Other Components ---\n",
    "        compute_metrics=compute_metrics, # Your function to compute ROUGE scores\n",
    "        tokenizer=processor.tokenizer    # Crucial for saving model correctly & maybe generation\n",
    "    )\n",
    "\n",
    "    print(\"\\nSeq2SeqTrainer object initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ERROR Initializing Trainer ---\")\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    print(\"Ensure model, args, collator, tokenizer/processor, compute_metrics are correctly defined.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
