{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate # For metrics\n",
    "from datasets import DatasetDict, Audio, Dataset\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "CSV_PATH = (DATA_DIR / \"musiccaps-public.csv\").resolve()\n",
    "AUDIO_DIR = DATA_DIR / \"audio\" \n",
    "AUDIO_EXTENSION = \".wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading metadata using pandas...\")\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"CSV loaded successfully with pandas. Shape: {df.shape}\")\n",
    "\n",
    "    df['audio_path'] = [str(AUDIO_DIR / f\"audio_{index}{AUDIO_EXTENSION}\") for index in df.index]\n",
    "    \n",
    "    # Convert the pandas DataFrame to a datasets.Dataset object\n",
    "    full_dataset = Dataset.from_pandas(df)\n",
    "    print(\"Converted pandas DataFrame to datasets.Dataset object:\")\n",
    "\n",
    "    print(full_dataset)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR (pandas): File not found during read_csv at {CSV_PATH}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Failed during pandas load or Dataset conversion from {CSV_PATH}:\")\n",
    "    raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFiltering dataset for existing audio files...\")\n",
    "\n",
    "def check_audio_file_exists(example):\n",
    "    from pathlib import Path\n",
    "    return Path(example['audio_path']).exists()\n",
    "\n",
    "initial_count = len(full_dataset)\n",
    "dataset_with_audio = full_dataset.filter(check_audio_file_exists, num_proc=4)\n",
    "final_count = len(dataset_with_audio)\n",
    "print(f\"full dataset size: {final_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SAMPLING_RATE = 16000\n",
    "print(f\"\\nCasting audio column and resampling to {TARGET_SAMPLING_RATE}Hz...\")\n",
    "dataset_with_audio = dataset_with_audio.cast_column(\n",
    "    \"audio_path\", # The column containing paths\n",
    "    Audio(sampling_rate=TARGET_SAMPLING_RATE) # Target object type and sampling rate\n",
    ")\n",
    "dataset_with_audio = dataset_with_audio.rename_column(\"audio_path\", \"audio\")\n",
    "print(dataset_with_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"openai/whisper-small\" # Or \"openai/whisper-base\", etc.\n",
    "LANGUAGE = \"English\" \n",
    "TASK = \"transcribe\" \n",
    "\n",
    "try:\n",
    "    # Load the feature extractor (processes audio input)\n",
    "    # We load it now even though we skipped audio loading, as it's part of the processor bundle\n",
    "    print(\"Loading feature extractor...\")\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "\n",
    "    print(\"Loading processor...\")\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "\n",
    "    print(\"\\nProcessor components loaded successfully:\")\n",
    "    print(f\"  Feature Extractor: {type(feature_extractor)}\")\n",
    "    print(f\"  Tokenizer: {type(tokenizer)}\")\n",
    "    print(f\"  Processor: {type(processor)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    \"\"\"\n",
    "    Prepares a batch of data for the Whisper model.\n",
    "\n",
    "    Input batch EXPECTS:\n",
    "      - An 'audio' column: Containing dictionaries from datasets.Audio. Example:\n",
    "          {'path': '...', 'array': numpy.ndarray, 'sampling_rate': 16000}\n",
    "      - A 'caption' column: Containing the text transcriptions/captions.\n",
    "\n",
    "    Output batch CONTAINS:\n",
    "      - 'input_features': Processed audio data (log-Mel spectrogram) for the model.\n",
    "      - 'labels': Tokenized text caption IDs for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    audio_data = batch[\"audio\"]\n",
    "    # Extract log-Mel spectrogram features from the raw audio array\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        audio_data[\"array\"], sampling_rate=audio_data[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "    # Note: We take [0] because feature_extractor processes one sample at a time\n",
    "        \n",
    "    # --- 2. Process Text ---\n",
    "    captions = batch[\"caption\"]\n",
    "    current_caption = captions \n",
    "    processed_caption = \"\" if None else str(current_caption)\n",
    "\n",
    "    batch[\"labels\"] = tokenizer(processed_caption).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nApplying dataset preparation function...\")\n",
    "prepared_dataset = dataset_with_audio.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=dataset_with_audio.column_names, # Remove old columns\n",
    "    num_proc=1\n",
    ")\n",
    "print(\"Dataset preparation complete.\")\n",
    "print(prepared_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and evaluation sets (e.g., 90% train, 10% test)\n",
    "print(\"\\nSplitting dataset into train and test sets...\")\n",
    "dataset_splits = prepared_dataset.train_test_split(test_size=0.1, seed=456)\n",
    "train_dataset = dataset_splits[\"train\"]\n",
    "eval_dataset = dataset_splits[\"test\"]\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Initialize Data Collator ---\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import dataclasses\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Custom data collator that correctly pads `input_features` and `labels`\n",
    "    for speech-to-text models using the processor.\n",
    "    \"\"\"\n",
    "    processor: Any # Should be WhisperProcessor or similar\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths\n",
    "        # and need different padding methods.\n",
    "        # `features` is a list of dicts, e.g. [{'input_features': ..., 'labels': ...}, ...]\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features] # Use 'input_ids' key for tokenizer padding\n",
    "\n",
    "        # Pad the audio features (input_features)\n",
    "        # The feature_extractor's `pad` method handles this correctly\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Pad the text labels (labels)\n",
    "        # The tokenizer's `pad` method handles this correctly\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace tokenizer's pad_token_id with -100 in labels to ignore padding in loss calculation\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If the model expects decoder_input_ids, create them by shifting labels\n",
    "        # (Whisper models typically handle this internally when labels are provided)\n",
    "        # batch[\"decoder_input_ids\"] = shift_tokens_right(labels, self.processor.tokenizer.pad_token_id)\n",
    "        # Note: Usually not needed explicitly for WhisperForConditionalGeneration if labels are passed\n",
    "\n",
    "        # Add the padded labels to the batch dictionary\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "print(\"\\nInitializing Custom Data Collator...\")\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "print(\"Custom Data Collator initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Evaluation Metrics ---\n",
    "print(\"\\nLoading ROUGE metric...\")\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes ROUGE scores from model predictions.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Decode labels, replacing -100 padding token\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Simple cleaning\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    # Compute ROUGE\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Multiply scores by 100\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    # Round results\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "print(\"Compute metrics function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import torch # Usually needed implicitly by transformers\n",
    "\n",
    "print(f\"\\nLoading model '{MODEL_NAME}'...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.config.use_cache = True # Enable cache for generation\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    print(\"Model moved to GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU detected. Model running on CPU.\")\n",
    "\n",
    "print(f\"Model '{MODEL_NAME}' loaded successfully on {model.device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# --- Step 10: Define Training Arguments ---\n",
    "print(\"\\nDefining Training Arguments...\")\n",
    "OUTPUT_DIR_MODEL = \"./whisper-musiccaps-finetuned-local\" # Model output directory\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_MODEL,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8, # Lower if you encounter CUDA Out-of-Memory errors\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-5, # $[1 \\times 10^{-5}]$\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_dir=f\"{OUTPUT_DIR_MODEL}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(), # Enable mixed precision only if GPU is available\n",
    "    gradient_accumulation_steps=2, # Effective batch size = 8 * num_gpus * 2\n",
    "    # gradient_checkpointing=True, # Uncomment if memory is very limited (slows training)\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\", # Or \"eval_rougeL\" etc. if using ROUGE\n",
    "    greater_is_better=False, # False for loss, True for ROUGE\n",
    "    remove_unused_columns=False, # Important: Keep as False after custom processing\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=[\"tensorboard\"],\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "print(\"Training Arguments defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "print(\"\\nInitializing Trainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    # --- Pass the ACTUAL datasets ---\n",
    "    train_dataset=train_dataset, # Use the prepared training data\n",
    "    eval_dataset=eval_dataset,   # Use the prepared evaluation data\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.tokenizer # Pass tokenizer for proper saving\n",
    ")\n",
    "print(\"Trainer initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 12: Start Training ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    trainer.save_model() # Saves the tokenizer too\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yt-dlp --extract-audio --postprocessor-args \"-ss 00:05:00 -to 00:05:10\" https://www.youtube.com/watch?v=_-kssA-FOzU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, target_sr=16000):\n",
    "    waveform, sr = torchaudio.load(path)  # shape: [channels, samples]\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != target_sr:\n",
    "        waveform = torchaudio.transforms.Resample(sr, target_sr)(waveform)\n",
    "\n",
    "    return waveform.squeeze()  # shape: [num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "checkpoint_path = Path(\"checkpoint-351\").resolve()  # absolute + OS-safe path\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path)\n",
    "processor = WhisperProcessor.from_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"./checkpoint-351\", local_files_only=True\n",
    ")\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"./checkpoint-351\", local_files_only=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"C:/Users/erics/OneDrive/Desktop/transcriber_server/fine_tuned_model/\")\n",
    "processor.save_pretrained(\"C:/Users/erics/OneDrive/Desktop/transcriber_server/fine_tuned_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Load the processor from the original Whisper Small model\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# # Load your fine-tuned model from your checkpoint directory\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\"checkpoint-351\", local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"/checkpoint-351\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "# Load the original processor (with language and task settings if needed)\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")\n",
    "\n",
    "# Save the processor's configuration into your checkpoint folder.\n",
    "# This will add files such as preprocessor_config.json to the folder.\n",
    "processor.save_pretrained(\"./whisper-musiccaps-finetuned-local/checkpoint351\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Now load the processor and model locally from your updated checkpoint folder.\n",
    "processor = WhisperProcessor.from_pretrained(\"checkpoint351\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"checkpoint351\").to(device)\n",
    "\n",
    "# (Optional) Test by printing the types:\n",
    "print(\"Processor loaded as:\", type(processor))\n",
    "print(\"Model loaded as:\", type(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.decoder.embed_positions.weight: torch.Size([448, 768]), dtype: torch.float32\n",
      "model.decoder.embed_tokens.weight: torch.Size([51865, 768]), dtype: torch.float32\n",
      "model.decoder.layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.0.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.0.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.0.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.1.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.1.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.1.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.1.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.10.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.10.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.10.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.10.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.10.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.10.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.10.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.10.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.11.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.11.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.11.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.11.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.11.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.11.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.11.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.11.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.2.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.2.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.2.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.2.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.3.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.3.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.3.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.3.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.4.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.4.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.4.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.4.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.5.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.5.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.5.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.5.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.6.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.6.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.6.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.6.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.6.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.6.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.6.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.6.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.7.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.7.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.7.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.7.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.7.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.7.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.7.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.7.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.8.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.8.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.8.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.8.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.8.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.8.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.8.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.8.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.encoder_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.9.encoder_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.encoder_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.9.encoder_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.encoder_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.9.encoder_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.encoder_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.decoder.layers.9.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.decoder.layers.9.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.decoder.layers.9.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.decoder.layers.9.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.decoder.layers.9.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.conv1.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.conv1.weight: torch.Size([768, 80, 3]), dtype: torch.float32\n",
      "model.encoder.conv2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.conv2.weight: torch.Size([768, 768, 3]), dtype: torch.float32\n",
      "model.encoder.embed_positions.weight: torch.Size([1500, 768]), dtype: torch.float32\n",
      "model.encoder.layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.0.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.0.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.0.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.0.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.0.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.0.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.0.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.1.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.1.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.1.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.1.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.1.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.1.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.1.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.10.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.10.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.10.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.10.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.10.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.10.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.10.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.10.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.11.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.11.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.11.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.11.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.11.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.11.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.11.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.11.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.2.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.2.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.2.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.2.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.2.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.2.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.2.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.2.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.3.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.3.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.3.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.3.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.3.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.3.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.3.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.3.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.4.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.4.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.4.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.4.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.4.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.4.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.4.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.4.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.5.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.5.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.5.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.5.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.5.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.5.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.5.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.5.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.6.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.6.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.6.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.6.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.6.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.6.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.6.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.6.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.7.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.7.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.7.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.7.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.7.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.7.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.7.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.7.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.8.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.8.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.8.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.8.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.8.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.8.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.8.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.8.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.9.fc1.bias: torch.Size([3072]), dtype: torch.float32\n",
      "model.encoder.layers.9.fc1.weight: torch.Size([3072, 768]), dtype: torch.float32\n",
      "model.encoder.layers.9.fc2.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.9.fc2.weight: torch.Size([768, 3072]), dtype: torch.float32\n",
      "model.encoder.layers.9.final_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.9.final_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), dtype: torch.float32\n",
      "model.encoder.layers.9.self_attn_layer_norm.bias: torch.Size([768]), dtype: torch.float32\n",
      "model.encoder.layers.9.self_attn_layer_norm.weight: torch.Size([768]), dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "# Replace with your safetensors file path.\n",
    "file_path = \"C:/Users/erics/OneDrive/Desktop/several-ducks-datahacks25/music_analyzer/whisper-musiccaps-finetuned-local/model.safetensors\"\n",
    "\n",
    "# Load the file.\n",
    "state_dict = load_file(file_path, device=\"cpu\")\n",
    "\n",
    "# Print out the keys and shapes.\n",
    "for key, tensor in state_dict.items():\n",
    "    print(f\"{key}: {tensor.shape}, dtype: {tensor.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.decoder.embed_positions.weight -> decoder.positional_embedding.weight\n",
      "model.decoder.embed_tokens.weight -> decoder.token_embedding.weight\n",
      "model.decoder.layer_norm.bias -> decoder.ln.bias\n",
      "model.decoder.layer_norm.weight -> decoder.ln.weight\n"
     ]
    }
   ],
   "source": [
    "def remap_key(old_key: str) -> str:\n",
    "    # Remove the \"model.\" prefix if present.\n",
    "    if old_key.startswith(\"model.\"):\n",
    "        old_key = old_key[len(\"model.\"):]\n",
    "    \n",
    "    # Rename \"embed_positions\" to \"positional_embedding\"\n",
    "    old_key = old_key.replace(\"embed_positions\", \"positional_embedding\")\n",
    "    \n",
    "    # Rename \"embed_tokens\" to \"token_embedding\"\n",
    "    old_key = old_key.replace(\"embed_tokens\", \"token_embedding\")\n",
    "    \n",
    "    # Optionally, if there are differences in normalization naming,\n",
    "    # replace \"layer_norm\" with \"ln\"\n",
    "    old_key = old_key.replace(\"layer_norm\", \"ln\")\n",
    "    \n",
    "    return old_key\n",
    "\n",
    "# Example: print a few remapped keys.\n",
    "checkpoint_keys = [\n",
    "    \"model.decoder.embed_positions.weight\",\n",
    "    \"model.decoder.embed_tokens.weight\",\n",
    "    \"model.decoder.layer_norm.bias\",\n",
    "    \"model.decoder.layer_norm.weight\"\n",
    "]\n",
    "\n",
    "for key in checkpoint_keys:\n",
    "    print(f\"{key} -> {remap_key(key)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys after mapping: {'encoder.blocks.2.attn.value.bias', 'decoder.positional_embedding', 'decoder.blocks.6.attn.out.bias', 'encoder.blocks.7.attn_ln.bias', 'encoder.blocks.7.attn.value.bias', 'decoder.blocks.7.cross_attn_ln.bias', 'encoder.blocks.6.mlp.2.bias', 'decoder.blocks.7.attn_ln.bias', 'encoder.blocks.8.mlp.0.weight', 'decoder.blocks.10.cross_attn_ln.weight', 'decoder.blocks.5.attn.query.weight', 'encoder.blocks.0.mlp.0.bias', 'decoder.blocks.11.cross_attn.key.weight', 'encoder.blocks.3.attn_ln.bias', 'decoder.blocks.9.cross_attn.value.weight', 'encoder.blocks.10.attn.query.weight', 'decoder.blocks.0.cross_attn.value.weight', 'encoder.blocks.10.attn.value.bias', 'encoder.blocks.4.attn.value.weight', 'encoder.blocks.9.attn_ln.bias', 'encoder.blocks.10.attn.value.weight', 'encoder.blocks.3.attn.query.bias', 'decoder.blocks.7.cross_attn.query.weight', 'decoder.blocks.8.cross_attn.query.bias', 'decoder.blocks.1.cross_attn.query.weight', 'encoder.blocks.11.attn.out.weight', 'encoder.blocks.11.mlp.2.weight', 'encoder.blocks.9.attn_ln.weight', 'decoder.blocks.11.mlp.2.weight', 'encoder.blocks.3.mlp_ln.weight', 'encoder.blocks.6.attn.value.weight', 'decoder.blocks.2.attn.out.bias', 'encoder.blocks.0.mlp_ln.bias', 'decoder.blocks.1.cross_attn_ln.bias', 'decoder.blocks.8.cross_attn.out.bias', 'encoder.blocks.9.attn.query.bias', 'decoder.blocks.7.attn.value.weight', 'decoder.blocks.5.mlp.2.bias', 'decoder.blocks.8.cross_attn.query.weight', 'encoder.blocks.11.attn.value.bias', 'encoder.blocks.10.mlp_ln.bias', 'encoder.blocks.0.mlp_ln.weight', 'decoder.blocks.3.cross_attn.query.bias', 'decoder.blocks.9.attn.query.weight', 'encoder.blocks.9.attn.query.weight', 'decoder.blocks.1.attn.out.weight', 'decoder.blocks.10.cross_attn_ln.bias', 'encoder.blocks.1.mlp.2.bias', 'encoder.blocks.2.mlp_ln.weight', 'decoder.blocks.5.mlp.0.bias', 'encoder.blocks.2.attn_ln.weight', 'decoder.blocks.1.cross_attn.key.weight', 'decoder.blocks.3.mlp.2.bias', 'decoder.blocks.3.mlp.0.weight', 'decoder.blocks.9.attn.key.weight', 'encoder.blocks.3.mlp.2.bias', 'encoder.blocks.5.mlp.2.bias', 'encoder.blocks.9.mlp.0.weight', 'decoder.blocks.11.attn_ln.bias', 'decoder.blocks.10.cross_attn.value.weight', 'decoder.blocks.5.attn.out.bias', 'encoder.blocks.10.mlp.0.bias', 'encoder.blocks.7.attn.key.weight', 'decoder.blocks.10.attn.query.bias', 'decoder.blocks.1.cross_attn.value.bias', 'decoder.blocks.1.cross_attn_ln.weight', 'encoder.blocks.8.attn_ln.weight', 'encoder.blocks.7.mlp.0.bias', 'decoder.blocks.11.attn.value.bias', 'encoder.blocks.5.attn.value.bias', 'encoder.blocks.6.attn_ln.bias', 'decoder.blocks.4.attn.out.weight', 'encoder.blocks.0.attn.value.bias', 'encoder.blocks.11.mlp.0.bias', 'decoder.blocks.3.attn.key.weight', 'encoder.blocks.5.mlp_ln.bias', 'decoder.blocks.8.attn.out.weight', 'decoder.blocks.10.attn.query.weight', 'decoder.blocks.5.cross_attn_ln.weight', 'decoder.blocks.8.cross_attn.value.bias', 'encoder.blocks.7.attn.query.weight', 'encoder.blocks.0.attn.out.bias', 'decoder.blocks.9.mlp.0.weight', 'decoder.blocks.8.mlp.2.weight', 'decoder.blocks.9.cross_attn.key.weight', 'decoder.blocks.8.mlp.0.bias', 'encoder.blocks.2.mlp_ln.bias', 'decoder.blocks.3.attn.out.bias', 'decoder.blocks.2.cross_attn.query.bias', 'decoder.blocks.9.attn_ln.bias', 'decoder.blocks.10.attn.value.bias', 'decoder.blocks.1.attn.value.bias', 'decoder.blocks.3.mlp.2.weight', 'decoder.blocks.4.cross_attn.query.bias', 'decoder.blocks.10.cross_attn.out.weight', 'decoder.blocks.3.cross_attn.value.bias', 'encoder.blocks.9.attn.out.weight', 'decoder.blocks.3.cross_attn.key.weight', 'decoder.blocks.1.mlp_ln.bias', 'encoder.blocks.4.mlp_ln.weight', 'encoder.blocks.4.attn_ln.bias', 'decoder.blocks.8.cross_attn.out.weight', 'encoder.blocks.1.attn.value.bias', 'encoder.blocks.7.attn_ln.weight', 'encoder.blocks.6.attn_ln.weight', 'decoder.blocks.9.cross_attn.value.bias', 'encoder.blocks.5.mlp.0.bias', 'decoder.blocks.3.cross_attn.out.weight', 'encoder.blocks.8.mlp.2.bias', 'encoder.blocks.4.attn_ln.weight', 'encoder.blocks.4.mlp_ln.bias', 'decoder.blocks.7.cross_attn_ln.weight', 'decoder.blocks.11.attn.value.weight', 'encoder.blocks.4.mlp.0.weight', 'encoder.blocks.6.mlp.2.weight', 'decoder.blocks.5.mlp_ln.weight', 'encoder.blocks.10.mlp.2.weight', 'encoder.blocks.11.mlp.2.bias', 'decoder.blocks.10.mlp_ln.bias', 'decoder.blocks.0.attn.value.bias', 'encoder.blocks.11.attn.value.weight', 'encoder.blocks.4.mlp.2.bias', 'decoder.blocks.4.mlp_ln.weight', 'encoder.blocks.5.attn.out.weight', 'decoder.blocks.2.attn.key.weight', 'encoder.positional_embedding', 'decoder.blocks.6.cross_attn.out.weight', 'decoder.blocks.11.cross_attn.query.weight', 'decoder.blocks.5.cross_attn.value.weight', 'encoder.blocks.9.mlp_ln.weight', 'decoder.blocks.2.attn_ln.bias', 'decoder.blocks.11.cross_attn_ln.bias', 'encoder.blocks.5.attn_ln.bias', 'encoder.blocks.9.mlp.2.bias', 'decoder.blocks.10.mlp_ln.weight', 'encoder.blocks.9.attn.key.weight', 'decoder.blocks.3.attn.value.bias', 'decoder.blocks.8.mlp.0.weight', 'decoder.blocks.4.attn.value.weight', 'decoder.blocks.5.cross_attn.query.bias', 'decoder.blocks.5.cross_attn.key.weight', 'decoder.blocks.8.mlp_ln.weight', 'encoder.blocks.5.attn.out.bias', 'decoder.blocks.4.mlp.0.weight', 'decoder.blocks.10.attn.key.weight', 'decoder.blocks.5.attn_ln.weight', 'decoder.blocks.10.attn.out.weight', 'decoder.blocks.3.mlp.0.bias', 'decoder.blocks.4.cross_attn.out.bias', 'decoder.blocks.9.attn.value.weight', 'decoder.blocks.10.cross_attn.key.weight', 'decoder.blocks.9.attn_ln.weight', 'encoder.blocks.10.attn.key.weight', 'decoder.blocks.7.attn.value.bias', 'decoder.blocks.2.attn.value.bias', 'encoder.blocks.3.mlp.0.bias', 'encoder.blocks.8.mlp.0.bias', 'encoder.blocks.1.attn.out.weight', 'encoder.blocks.0.attn.out.weight', 'decoder.blocks.5.attn.value.weight', 'decoder.blocks.1.mlp.2.bias', 'encoder.ln_post.bias', 'decoder.blocks.11.attn.out.weight', 'encoder.blocks.3.attn.key.weight', 'decoder.blocks.4.mlp.2.weight', 'decoder.blocks.7.cross_attn.out.weight', 'decoder.blocks.0.mlp.0.bias', 'encoder.blocks.3.attn.out.bias', 'decoder.blocks.6.cross_attn.query.bias', 'decoder.blocks.1.mlp.0.weight', 'decoder.blocks.8.cross_attn_ln.weight', 'encoder.blocks.10.attn.out.weight', 'decoder.blocks.2.attn.out.weight', 'decoder.blocks.0.mlp_ln.bias', 'decoder.blocks.11.mlp.0.bias', 'decoder.blocks.7.cross_attn.value.weight', 'decoder.blocks.1.attn_ln.weight', 'decoder.blocks.3.attn_ln.bias', 'decoder.blocks.11.cross_attn_ln.weight', 'decoder.blocks.11.mlp.2.bias', 'encoder.blocks.8.attn.value.weight', 'decoder.blocks.8.attn.out.bias', 'decoder.blocks.9.cross_attn_ln.bias', 'decoder.blocks.4.attn.out.bias', 'encoder.ln_post.weight', 'decoder.blocks.4.mlp.2.bias', 'decoder.blocks.5.cross_attn.out.weight', 'encoder.blocks.8.mlp_ln.bias', 'encoder.blocks.3.mlp.2.weight', 'decoder.blocks.11.attn_ln.weight', 'decoder.blocks.10.attn_ln.weight', 'decoder.blocks.3.attn.out.weight', 'decoder.blocks.3.cross_attn_ln.weight', 'decoder.blocks.5.mlp_ln.bias', 'decoder.blocks.6.mlp.0.weight', 'encoder.blocks.0.mlp.2.bias', 'encoder.blocks.11.mlp_ln.weight', 'decoder.blocks.8.attn.value.bias', 'encoder.blocks.0.attn_ln.bias', 'encoder.blocks.1.attn.query.weight', 'encoder.blocks.0.attn.value.weight', 'encoder.blocks.5.mlp.2.weight', 'decoder.blocks.4.cross_attn.out.weight', 'decoder.blocks.10.mlp.2.bias', 'encoder.blocks.5.attn.key.weight', 'encoder.blocks.6.attn.query.weight', 'decoder.blocks.1.cross_attn.out.bias', 'decoder.blocks.4.cross_attn.query.weight', 'encoder.blocks.6.mlp_ln.weight', 'decoder.blocks.1.attn.key.weight', 'decoder.blocks.5.mlp.2.weight', 'encoder.blocks.2.attn.out.weight', 'encoder.blocks.7.attn.query.bias', 'encoder.blocks.1.mlp_ln.weight', 'decoder.blocks.11.cross_attn.value.weight', 'encoder.blocks.5.mlp.0.weight', 'decoder.blocks.2.cross_attn.query.weight', 'decoder.blocks.8.mlp.2.bias', 'decoder.blocks.9.cross_attn.query.bias', 'encoder.blocks.9.mlp.0.bias', 'decoder.blocks.10.cross_attn.query.bias', 'decoder.blocks.6.cross_attn.out.bias', 'decoder.blocks.5.attn.value.bias', 'decoder.blocks.2.cross_attn_ln.bias', 'decoder.blocks.3.attn_ln.weight', 'decoder.blocks.8.cross_attn.key.weight', 'encoder.blocks.0.attn.query.weight', 'decoder.blocks.6.attn_ln.bias', 'decoder.blocks.7.cross_attn.query.bias', 'encoder.blocks.10.mlp.2.bias', 'encoder.blocks.4.mlp.2.weight', 'encoder.blocks.6.attn.key.weight', 'encoder.blocks.6.attn.out.bias', 'decoder.blocks.11.cross_attn.query.bias', 'encoder.blocks.2.attn.key.weight', 'decoder.blocks.4.attn_ln.weight', 'decoder.blocks.0.mlp.2.weight', 'decoder.blocks.1.cross_attn.out.weight', 'decoder.blocks.0.cross_attn.value.bias', 'encoder.blocks.4.attn.key.weight', 'encoder.blocks.11.attn_ln.weight', 'decoder.blocks.7.attn.out.weight', 'encoder.blocks.2.attn_ln.bias', 'decoder.blocks.1.mlp.2.weight', 'decoder.blocks.10.attn.out.bias', 'decoder.blocks.0.cross_attn.out.weight', 'decoder.blocks.4.cross_attn.key.weight', 'encoder.blocks.10.attn_ln.weight', 'encoder.blocks.1.mlp.0.bias', 'encoder.blocks.4.mlp.0.bias', 'decoder.blocks.7.cross_attn.out.bias', 'decoder.blocks.0.cross_attn.query.weight', 'decoder.blocks.2.cross_attn.out.bias', 'decoder.blocks.8.cross_attn_ln.bias', 'encoder.blocks.1.attn.key.weight', 'decoder.blocks.1.attn.query.bias', 'decoder.blocks.11.cross_attn.out.bias', 'decoder.blocks.2.mlp.2.bias', 'encoder.blocks.11.attn.query.weight', 'decoder.blocks.11.mlp_ln.bias', 'decoder.blocks.2.cross_attn.key.weight', 'decoder.blocks.6.attn_ln.weight', 'decoder.blocks.10.attn.value.weight', 'decoder.blocks.7.mlp.2.weight', 'encoder.blocks.0.attn_ln.weight', 'encoder.blocks.1.mlp.2.weight', 'encoder.blocks.8.attn.out.bias', 'decoder.blocks.2.mlp.0.bias', 'decoder.blocks.9.attn.query.bias', 'encoder.blocks.11.mlp_ln.bias', 'encoder.blocks.11.mlp.0.weight', 'decoder.blocks.4.mlp_ln.bias', 'decoder.blocks.3.mlp_ln.weight', 'encoder.blocks.11.attn.query.bias', 'decoder.blocks.7.attn.query.weight', 'encoder.blocks.3.attn.out.weight', 'decoder.blocks.9.mlp_ln.weight', 'decoder.blocks.6.cross_attn_ln.bias', 'decoder.blocks.0.cross_attn_ln.weight', 'encoder.blocks.8.mlp_ln.weight', 'decoder.blocks.3.attn.value.weight', 'decoder.blocks.0.mlp_ln.weight', 'decoder.blocks.10.cross_attn.value.bias', 'encoder.blocks.9.attn.out.bias', 'encoder.blocks.1.mlp.0.weight', 'decoder.blocks.6.cross_attn_ln.weight', 'decoder.blocks.11.attn.query.weight', 'decoder.blocks.7.attn_ln.weight', 'decoder.blocks.4.attn_ln.bias', 'decoder.blocks.9.cross_attn_ln.weight', 'decoder.blocks.6.attn.out.weight', 'encoder.blocks.3.attn.value.bias', 'decoder.blocks.0.cross_attn.out.bias', 'decoder.blocks.3.attn.query.bias', 'decoder.blocks.7.mlp_ln.bias', 'encoder.blocks.3.mlp_ln.bias', 'decoder.blocks.0.attn_ln.weight', 'decoder.blocks.0.attn.key.weight', 'encoder.blocks.6.mlp.0.bias', 'encoder.blocks.0.attn.query.bias', 'decoder.blocks.4.attn.key.weight', 'decoder.blocks.8.attn.query.weight', 'decoder.blocks.6.mlp.2.bias', 'decoder.blocks.6.attn.value.weight', 'encoder.blocks.9.attn.value.bias', 'encoder.blocks.5.attn_ln.weight', 'decoder.blocks.11.attn.key.weight', 'decoder.blocks.1.attn.value.weight', 'encoder.blocks.3.mlp.0.weight', 'decoder.blocks.4.cross_attn_ln.bias', 'decoder.blocks.8.attn_ln.weight', 'decoder.blocks.0.cross_attn_ln.bias', 'decoder.blocks.3.cross_attn_ln.bias', 'decoder.blocks.3.attn.query.weight', 'encoder.blocks.9.attn.value.weight', 'decoder.blocks.7.cross_attn.value.bias', 'decoder.blocks.0.cross_attn.query.bias', 'decoder.blocks.7.mlp_ln.weight', 'encoder.blocks.6.attn.value.bias', 'encoder.blocks.1.attn_ln.weight', 'encoder.blocks.5.attn.query.weight', 'decoder.blocks.0.attn.query.bias', 'decoder.blocks.11.cross_attn.out.weight', 'decoder.blocks.11.mlp.0.weight', 'decoder.blocks.9.mlp.0.bias', 'encoder.blocks.11.attn.out.bias', 'encoder.blocks.8.attn.query.bias', 'encoder.blocks.3.attn_ln.weight', 'decoder.blocks.3.cross_attn.query.weight', 'decoder.blocks.6.cross_attn.value.weight', 'encoder.blocks.9.mlp.2.weight', 'encoder.blocks.0.mlp.2.weight', 'encoder.blocks.8.attn.key.weight', 'decoder.blocks.6.mlp_ln.weight', 'decoder.blocks.2.attn_ln.weight', 'decoder.blocks.2.cross_attn.value.weight', 'decoder.blocks.5.cross_attn_ln.bias', 'decoder.blocks.6.cross_attn.value.bias', 'encoder.blocks.1.attn_ln.bias', 'encoder.blocks.10.attn.query.bias', 'decoder.blocks.4.attn.query.weight', 'decoder.blocks.7.attn.query.bias', 'encoder.blocks.5.mlp_ln.weight', 'decoder.blocks.11.mlp_ln.weight', 'encoder.blocks.4.attn.query.bias', 'encoder.blocks.2.mlp.0.bias', 'encoder.blocks.10.attn.out.bias', 'decoder.blocks.4.mlp.0.bias', 'encoder.blocks.8.attn_ln.bias', 'decoder.blocks.7.mlp.0.bias', 'encoder.blocks.5.attn.value.weight', 'decoder.blocks.7.mlp.2.bias', 'decoder.blocks.3.cross_attn.value.weight', 'decoder.blocks.3.mlp_ln.bias', 'encoder.blocks.2.mlp.2.bias', 'decoder.blocks.8.attn_ln.bias', 'encoder.blocks.11.attn.key.weight', 'decoder.blocks.11.cross_attn.value.bias', 'decoder.blocks.2.attn.value.weight', 'decoder.blocks.6.cross_attn.key.weight', 'encoder.blocks.7.attn.out.weight', 'decoder.blocks.8.cross_attn.value.weight', 'encoder.blocks.7.attn.value.weight', 'decoder.blocks.2.mlp.2.weight', 'decoder.blocks.11.attn.out.bias', 'encoder.blocks.6.attn.query.bias', 'decoder.blocks.4.attn.query.bias', 'encoder.blocks.8.mlp.2.weight', 'decoder.blocks.5.attn.query.bias', 'decoder.blocks.9.mlp.2.weight', 'decoder.blocks.9.cross_attn.out.weight', 'decoder.blocks.6.mlp.2.weight', 'encoder.blocks.8.attn.out.weight', 'decoder.blocks.0.mlp.2.bias', 'decoder.blocks.1.cross_attn.value.weight', 'encoder.blocks.7.attn.out.bias', 'decoder.blocks.2.cross_attn_ln.weight', 'decoder.blocks.6.mlp.0.bias', 'decoder.blocks.2.attn.query.bias', 'decoder.blocks.5.attn.out.weight', 'decoder.blocks.0.cross_attn.key.weight', 'decoder.blocks.8.attn.value.weight', 'decoder.blocks.5.attn.key.weight', 'decoder.blocks.5.mlp.0.weight', 'encoder.blocks.1.mlp_ln.bias', 'decoder.blocks.10.cross_attn.out.bias', 'decoder.blocks.3.cross_attn.out.bias', 'decoder.blocks.1.attn.query.weight', 'decoder.blocks.9.cross_attn.query.weight', 'decoder.blocks.10.cross_attn.query.weight', 'encoder.blocks.2.attn.query.weight', 'encoder.blocks.3.attn.value.weight', 'encoder.blocks.7.mlp_ln.bias', 'decoder.blocks.0.mlp.0.weight', 'encoder.blocks.2.attn.query.bias', 'encoder.blocks.4.attn.query.weight', 'encoder.blocks.4.attn.out.bias', 'decoder.blocks.9.mlp_ln.bias', 'decoder.blocks.9.attn.out.weight', 'encoder.blocks.10.mlp.0.weight', 'decoder.blocks.7.attn.key.weight', 'decoder.blocks.5.cross_attn.query.weight', 'decoder.blocks.6.attn.key.weight', 'decoder.blocks.1.mlp.0.bias', 'decoder.blocks.9.attn.out.bias', 'encoder.blocks.10.attn_ln.bias', 'decoder.blocks.0.attn.value.weight', 'decoder.blocks.1.mlp_ln.weight', 'encoder.blocks.1.attn.out.bias', 'decoder.blocks.8.mlp_ln.bias', 'decoder.blocks.2.mlp.0.weight', 'decoder.blocks.2.attn.query.weight', 'decoder.blocks.4.cross_attn.value.weight', 'decoder.blocks.4.cross_attn_ln.weight', 'encoder.blocks.0.mlp.0.weight', 'encoder.blocks.2.attn.out.bias', 'decoder.blocks.6.attn.value.bias', 'decoder.blocks.0.attn.out.bias', 'decoder.blocks.1.cross_attn.query.bias', 'encoder.blocks.4.attn.out.weight', 'decoder.blocks.9.mlp.2.bias', 'decoder.blocks.11.attn.query.bias', 'decoder.blocks.10.mlp.0.bias', 'decoder.blocks.7.mlp.0.weight', 'encoder.blocks.2.mlp.2.weight', 'decoder.blocks.1.attn.out.bias', 'encoder.blocks.8.attn.value.bias', 'encoder.blocks.3.attn.query.weight', 'encoder.blocks.2.attn.value.weight', 'encoder.blocks.6.mlp_ln.bias', 'decoder.blocks.6.attn.query.bias', 'encoder.blocks.6.mlp.0.weight', 'decoder.blocks.7.attn.out.bias', 'encoder.blocks.6.attn.out.weight', 'decoder.blocks.9.attn.value.bias', 'decoder.blocks.5.cross_attn.value.bias', 'decoder.blocks.4.attn.value.bias', 'decoder.blocks.2.mlp_ln.weight', 'encoder.blocks.0.attn.key.weight', 'encoder.blocks.7.mlp_ln.weight', 'decoder.blocks.0.attn_ln.bias', 'encoder.blocks.9.mlp_ln.bias', 'decoder.blocks.2.mlp_ln.bias', 'encoder.blocks.4.attn.value.bias', 'decoder.blocks.10.attn_ln.bias', 'encoder.blocks.7.mlp.0.weight', 'encoder.blocks.10.mlp_ln.weight', 'encoder.blocks.5.attn.query.bias', 'decoder.blocks.6.cross_attn.query.weight', 'encoder.blocks.7.mlp.2.bias', 'decoder.blocks.1.attn_ln.bias', 'encoder.blocks.1.attn.value.weight', 'decoder.blocks.10.mlp.2.weight', 'encoder.blocks.11.attn_ln.bias', 'decoder.blocks.2.cross_attn.value.bias', 'decoder.blocks.6.attn.query.weight', 'decoder.blocks.5.attn_ln.bias', 'decoder.blocks.9.cross_attn.out.bias', 'encoder.blocks.2.mlp.0.weight', 'encoder.blocks.1.attn.query.bias', 'decoder.blocks.10.mlp.0.weight', 'decoder.blocks.0.attn.out.weight', 'decoder.blocks.8.attn.query.bias', 'decoder.blocks.6.mlp_ln.bias', 'decoder.blocks.4.cross_attn.value.bias', 'decoder.blocks.0.attn.query.weight', 'decoder.blocks.8.attn.key.weight', 'encoder.blocks.7.mlp.2.weight', 'decoder.blocks.2.cross_attn.out.weight', 'decoder.blocks.5.cross_attn.out.bias', 'decoder.blocks.7.cross_attn.key.weight', 'encoder.blocks.8.attn.query.weight'}\n",
      "Unexpected keys after mapping: {'decoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.1.fc2.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.7.fc2.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'encoder.layers.8.fc2.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.bias', 'encoder.layers.6.final_ln.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.9.fc1.weight', 'decoder.layers.11.self_attn_ln.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'decoder.layers.3.final_ln.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.8.encoder_attn_ln.bias', 'encoder.layers.10.self_attn_ln.bias', 'decoder.layers.0.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.7.fc2.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.6.fc2.bias', 'encoder.layers.10.final_ln.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.3.self_attn_ln.bias', 'encoder.layers.9.self_attn_ln.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.4.fc1.weight', 'encoder.layers.6.fc1.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.weight', 'encoder.layers.11.fc1.bias', 'decoder.layers.5.self_attn_ln.weight', 'encoder.layers.8.self_attn.q_proj.weight', 'decoder.layers.8.self_attn.q_proj.weight', 'decoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.6.fc2.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.4.self_attn_ln.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.1.fc2.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'encoder.layers.2.fc1.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.8.self_attn.v_proj.bias', 'decoder.layers.6.fc1.bias', 'decoder.layers.3.encoder_attn_ln.weight', 'decoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.3.fc1.bias', 'encoder.layers.7.final_ln.weight', 'decoder.layers.7.encoder_attn_ln.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn_ln.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.4.final_ln.weight', 'encoder.layers.10.self_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.5.final_ln.bias', 'decoder.layers.3.encoder_attn_ln.bias', 'decoder.layers.6.encoder_attn_ln.bias', 'encoder.layers.11.final_ln.weight', 'encoder.layers.6.fc1.bias', 'decoder.layers.1.fc1.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.fc2.weight', 'decoder.layers.9.fc1.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.9.fc2.bias', 'decoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.4.final_ln.bias', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'encoder.layers.8.fc2.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.10.fc1.bias', 'decoder.layers.9.final_ln.weight', 'encoder.layers.11.final_ln.bias', 'decoder.layers.11.fc1.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn_ln.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.11.fc2.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.5.fc2.bias', 'decoder.layers.4.fc2.bias', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.5.final_ln.weight', 'encoder.layers.3.fc2.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.1.final_ln.weight', 'encoder.layers.10.self_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.0.fc2.bias', 'decoder.layers.10.encoder_attn_ln.weight', 'decoder.layers.11.fc1.weight', 'decoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.8.fc1.bias', 'encoder.layers.9.self_attn_ln.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.11.self_attn.out_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn_ln.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'encoder.layers.9.self_attn.q_proj.weight', 'decoder.layers.3.fc1.weight', 'encoder.layers.8.self_attn_ln.bias', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.0.final_ln.bias', 'decoder.layers.6.final_ln.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.fc1.bias', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.6.final_ln.bias', 'decoder.layers.3.fc2.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.4.fc1.weight', 'encoder.layers.5.self_attn_ln.weight', 'decoder.layers.11.fc2.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.7.fc1.bias', 'encoder.layers.0.self_attn_ln.bias', 'decoder.layers.10.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn_ln.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.9.final_ln.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.1.fc1.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'encoder.layers.5.fc1.bias', 'encoder.layers.4.fc2.weight', 'decoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.8.self_attn_ln.weight', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.10.fc2.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.2.encoder_attn_ln.weight', 'decoder.positional_embedding.weight', 'encoder.layers.5.self_attn_ln.bias', 'encoder.ln.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.6.self_attn_ln.bias', 'encoder.layers.2.self_attn_ln.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn_ln.weight', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'encoder.layers.1.self_attn_ln.bias', 'encoder.layers.7.self_attn_ln.weight', 'decoder.layers.8.final_ln.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.9.self_attn_ln.weight', 'decoder.layers.3.self_attn_ln.bias', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.7.fc2.weight', 'encoder.layers.1.self_attn_ln.weight', 'decoder.layers.3.fc1.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.4.fc1.bias', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_ln.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.2.self_attn_ln.bias', 'decoder.layers.7.self_attn_ln.bias', 'encoder.layers.1.final_ln.weight', 'encoder.layers.10.fc2.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.4.final_ln.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.7.final_ln.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.8.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.9.self_attn_ln.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.6.self_attn_ln.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.7.fc1.weight', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.3.self_attn_ln.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.10.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.4.self_attn_ln.bias', 'encoder.layers.9.final_ln.weight', 'decoder.layers.11.encoder_attn_ln.bias', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'decoder.layers.8.fc2.weight', 'decoder.layers.10.final_ln.bias', 'encoder.layers.2.final_ln.weight', 'encoder.layers.11.fc1.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'decoder.layers.9.encoder_attn_ln.weight', 'decoder.layers.6.encoder_attn_ln.weight', 'decoder.layers.2.final_ln.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.9.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn_ln.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'encoder.layers.11.self_attn.q_proj.weight', 'decoder.layers.9.self_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.6.self_attn_ln.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.9.self_attn.v_proj.bias', 'decoder.layers.5.self_attn_ln.bias', 'decoder.layers.9.self_attn.v_proj.weight', 'decoder.layers.1.encoder_attn_ln.bias', 'encoder.layers.4.self_attn_ln.weight', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.5.fc2.weight', 'decoder.layers.10.self_attn_ln.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.11.self_attn.q_proj.weight', 'decoder.layers.11.encoder_attn_ln.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.9.encoder_attn_ln.bias', 'decoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.2.final_ln.bias', 'decoder.layers.7.self_attn_ln.weight', 'decoder.layers.11.self_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.11.fc2.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.bias', 'encoder.layers.3.fc2.bias', 'decoder.layers.2.final_ln.weight', 'encoder.layers.9.fc1.bias', 'decoder.layers.8.fc1.bias', 'decoder.layers.10.fc1.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.10.self_attn.v_proj.bias', 'decoder.layers.4.final_ln.bias', 'decoder.layers.10.encoder_attn_ln.bias', 'decoder.layers.0.final_ln.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.final_ln.bias', 'decoder.layers.8.fc1.weight', 'decoder.layers.0.fc2.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.weight', 'encoder.layers.7.fc2.weight', 'encoder.layers.2.self_attn_ln.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.5.final_ln.bias', 'decoder.layers.7.final_ln.bias', 'encoder.layers.6.self_attn_ln.bias', 'encoder.layers.9.fc2.weight', 'decoder.layers.10.self_attn.v_proj.weight', 'decoder.layers.10.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.5.fc1.bias', 'decoder.layers.9.fc2.bias', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.6.fc2.weight', 'decoder.layers.8.self_attn.out_proj.bias', 'decoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.0.fc1.bias', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.8.fc2.bias', 'decoder.layers.6.fc1.weight', 'decoder.layers.8.self_attn_ln.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.11.self_attn_ln.bias', 'decoder.layers.1.self_attn_ln.weight', 'encoder.layers.4.fc2.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'encoder.layers.11.self_attn_ln.weight', 'encoder.layers.10.self_attn_ln.weight', 'decoder.layers.11.self_attn_ln.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.positional_embedding.weight', 'decoder.layers.10.encoder_attn.v_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'encoder.layers.0.final_ln.bias', 'encoder.layers.3.fc1.weight', 'decoder.layers.3.self_attn_ln.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.8.final_ln.weight', 'decoder.layers.1.self_attn_ln.bias', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.10.fc2.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn_ln.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.10.fc1.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.9.self_attn.out_proj.bias', 'decoder.layers.10.final_ln.weight', 'encoder.layers.7.fc1.weight', 'encoder.layers.9.self_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.11.fc2.bias', 'decoder.layers.8.encoder_attn_ln.weight', 'encoder.layers.3.final_ln.bias', 'encoder.layers.7.fc1.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.6.final_ln.bias', 'encoder.layers.0.final_ln.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.bias', 'encoder.layers.5.fc2.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.8.fc1.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'decoder.layers.10.fc2.bias', 'decoder.layers.11.final_ln.weight', 'decoder.layers.10.fc1.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.out_proj.weight', 'decoder.layers.3.final_ln.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.7.final_ln.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.5.final_ln.weight', 'decoder.layers.11.self_attn.out_proj.weight', 'decoder.layers.8.final_ln.bias', 'encoder.layers.1.final_ln.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'decoder.layers.2.fc2.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.weight', 'encoder.layers.10.final_ln.bias', 'decoder.layers.5.encoder_attn.v_proj.bias', 'encoder.layers.7.self_attn_ln.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.9.fc1.weight', 'decoder.layers.11.final_ln.bias', 'decoder.layers.9.fc2.weight', 'decoder.layers.11.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.4.encoder_attn_ln.bias', 'decoder.layers.0.self_attn_ln.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.2.fc1.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.1.fc2.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'decoder.layers.5.encoder_attn_ln.weight', 'encoder.layers.5.fc1.weight', 'decoder.layers.9.final_ln.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'encoder.ln.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.2.fc1.bias', 'encoder.layers.0.self_attn_ln.weight', 'encoder.layers.8.final_ln.bias', 'decoder.layers.8.self_attn_ln.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.0.encoder_attn_ln.weight', 'decoder.layers.7.encoder_attn_ln.weight', 'decoder.layers.10.self_attn_ln.weight', 'encoder.layers.6.fc2.weight', 'encoder.layers.3.final_ln.weight'}\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "# Load base model\n",
    "model = whisper.load_model(\"small\")\n",
    "base_keys = set(model.state_dict().keys())\n",
    "\n",
    "# Load your checkpoint state dict (from safetensors)\n",
    "checkpoint_path = \"C:/Users/erics/OneDrive/Desktop/several-ducks-datahacks25/music_analyzer/whisper-musiccaps-finetuned-local/model.safetensors\"\n",
    "ckpt_state = load_safetensors(checkpoint_path, device=\"cpu\")\n",
    "mapped_ckpt = {remap_key(k): v for k, v in ckpt_state.items()}\n",
    "\n",
    "mapped_keys = set(mapped_ckpt.keys())\n",
    "\n",
    "missing = base_keys - mapped_keys\n",
    "unexpected = mapped_keys - base_keys\n",
    "\n",
    "print(\"Missing keys after mapping:\", missing)\n",
    "print(\"Unexpected keys after mapping:\", unexpected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing in fine-tuned state: {'proj_out.weight'}\n",
      "Extra in fine-tuned state: set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the base model from the original source\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# Step 2: Load your fine-tuned checkpoint (safetensors)\n",
    "ft_state = load_safetensors(\"C:/Users/erics/OneDrive/Desktop/several-ducks-datahacks25/music_analyzer/whisper-musiccaps-finetuned-local/model.safetensors\", device=\"cpu\")\n",
    "\n",
    "# Optional: Inspect the keys of both state dicts if needed\n",
    "base_keys = set(base_model.state_dict().keys())\n",
    "ft_keys = set(ft_state.keys())\n",
    "print(\"Missing in fine-tuned state:\", base_keys - ft_keys)\n",
    "print(\"Extra in fine-tuned state:\", ft_keys - base_keys)\n",
    "\n",
    "# Step 3: Merge fine-tuned weights into the base model's state dict\n",
    "# This assumes that the keys now align; if they dont perfectly, you may need a remapping function.\n",
    "base_state = base_model.state_dict()\n",
    "base_state.update(ft_state)\n",
    "base_model.load_state_dict(base_state)\n",
    "\n",
    "# Now you can use `base_model` for inference or further fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "# Load the base model from openai/whisper-small.\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "base_state = base_model.state_dict()\n",
    "\n",
    "# Load your fine-tuned weights.\n",
    "ft_state = load_safetensors(\"C:/Users/erics/OneDrive/Desktop/several-ducks-datahacks25/music_analyzer/whisper-musiccaps-finetuned-local/model.safetensors\", device=\"cpu\")\n",
    "\n",
    "# If 'proj_out.weight' is missing, add it from the base state.\n",
    "if 'proj_out.weight' not in ft_state:\n",
    "    ft_state['proj_out.weight'] = base_state['proj_out.weight']\n",
    "\n",
    "# Merge the states and load them into the model.\n",
    "base_state.update(ft_state)\n",
    "base_model.load_state_dict(base_state)\n",
    "\n",
    "base_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: A male vocalist sings this melodious song. The tempo is slow with a lot of reverb and a lot of reverb. The song is melodic and emotional. The audio quality is poor.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "# Assume base_model is your fine-tuned model loaded earlier and is in eval mode.\n",
    "# For consistency, load the processor from the same source.\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# Load an audio file (ensure it's sampled at 16kHz for Whisper)\n",
    "audio_file = \"C:/Users/erics/OneDrive/Desktop/several-ducks-datahacks25/music_analyzer/wav_folder/audio_1682.wav\"\n",
    "audio, sr = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "# Use the processor's feature extractor to convert the audio waveform into input features.\n",
    "# The processor expects the audio to be a NumPy array and will return a tensor.\n",
    "inputs = processor.feature_extractor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "input_features = inputs.input_features  # shape: (batch_size, feature_dim, time)\n",
    "\n",
    "# If your model is on a GPU, make sure to move the inputs to the same device:\n",
    "input_features = input_features.to(base_model.device)\n",
    "\n",
    "# Generate model outputs. The generate() method will use the model's beam search or other decoding strategy.\n",
    "predicted_ids = base_model.generate(input_features)\n",
    "\n",
    "# Decode the predicted token IDs into text using the processor's tokenizer.\n",
    "transcription = processor.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
